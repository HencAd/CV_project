{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.io as io\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor, TimesformerForVideoClassification, BitsAndBytesConfig\n",
    "\n",
    "import accelerate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '679be20c3e94b783ac5be8a1', 'name': 'AddHe', 'fullname': 'Ada Henc', 'isPro': False, 'avatarUrl': '/avatars/436fa2e8d09fa03f9aa3b2a5e07695b0.svg', 'orgs': [{'type': 'org', 'id': '679bd7f5c8fcaa69de68bf61', 'name': 'cvproject', 'fullname': 'CV Project ', 'avatarUrl': 'https://www.gravatar.com/avatar/4cc3ce8cb0710b3102c755d0a736f86d?d=retro&size=100', 'roleInOrg': 'write', 'isEnterprise': False}], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'CV_project', 'role': 'fineGrained', 'createdAt': '2025-02-06T21:12:01.721Z', 'fineGrained': {'canReadGatedRepos': True, 'global': ['inference.serverless.write'], 'scoped': [{'entity': {'_id': '679bd7f5c8fcaa69de68bf61', 'type': 'org', 'name': 'cvproject'}, 'permissions': ['repo.content.read', 'repo.write', 'inference.endpoints.infer.write']}, {'entity': {'_id': '679be20c3e94b783ac5be8a1', 'type': 'user', 'name': 'AddHe'}, 'permissions': ['repo.content.read', 'repo.write']}]}}}}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import whoami\n",
    "print(whoami())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "def get_available_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():  # Dla Mac z Apple Silicon\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_available_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_frames(video: torch.Tensor, num_frames: int):\n",
    "    \"\"\"\n",
    "    Zakłada, że video ma kształt (C, T, H, W) i wybiera równomiernie rozłożone klatki.\n",
    "    \"\"\"\n",
    "    total_frames = video.shape[1]\n",
    "    if total_frames < num_frames:\n",
    "        # Jeżeli wideo ma mniej klatek, można uzupełnić paddingiem (np. powielając ostatnią klatkę) - jesli zdecydujemy sie na krotsze filmiki \n",
    "        pad = video[:, -1:, :, :].repeat(1, num_frames - total_frames, 1, 1)\n",
    "        video = torch.cat([video, pad], dim=1)\n",
    "        return video\n",
    "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    return video[:, indices, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdDetectionDataset(Dataset):\n",
    "    def __init__(self, root: str, split: str, transform=None, max_frames=100):\n",
    "        \"\"\"\n",
    "        Inicjalizuje dataset, ładuje ścieżki do plików wideo i przypisuje etykiety.\n",
    "\n",
    "        :param root: Główny katalog, w którym znajdują się dane (np. \"dataset\").\n",
    "        :param split: Określa, którą część danych ładować: 'train', 'validate', 'test'.\n",
    "        :param transform: Możliwość dodania transformacji do wideo.\n",
    "        :param max_frames: Maksymalna liczba klatek w sekwencji.\n",
    "        \"\"\"\n",
    "        self.root_dir = os.path.join(root, split)\n",
    "        self.transform = transform\n",
    "        self.max_frames = max_frames\n",
    "        self.video_paths = []\n",
    "        self.labels = []\n",
    "        self.idx_to_label = {0: 'content', 1: 'commercial'}\n",
    "\n",
    "        # Zbieranie plików wideo z folderów 'content' i 'commercial'\n",
    "        for i, label in self.idx_to_label.items():\n",
    "            label_dir_path = os.path.join(self.root_dir, label)\n",
    "            videos_in_dir = os.listdir(label_dir_path)\n",
    "            self.video_paths.extend([os.path.join(label_dir_path, video) for video in videos_in_dir])\n",
    "            self.labels.extend([i] * len(videos_in_dir))\n",
    "\n",
    "    def map_idx_to_label(self, idx):\n",
    "        \"\"\"Mapowanie indeksu na etykietę (np. 0 -> 'content', 1 -> 'commercial').\"\"\"\n",
    "        return self.idx_to_label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Zwraca liczbę elementów w zbiorze danych (liczba filmów).\"\"\"\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Wczytuje wideo, stosuje padding i zwraca tensor.\n",
    "\n",
    "        :param idx: Indeks pliku w zbiorze danych.\n",
    "        :returns: Para (wideo, etykieta).\n",
    "        \"\"\"\n",
    "            \n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Wczytanie pliku wideo\n",
    "        # video = self.load_video(video_path)\n",
    "        # video, _, _ = io.read_video(video_path, pts_unit=\"sec\")  # Zwraca tensor (T, H, W, C)\n",
    "\n",
    "        video, _, _ = io.read_video(video_path, pts_unit=\"sec\")\n",
    "        video = video.permute(3, 0, 1, 2).float() / 255.0  # (T, H, W, C) -> (C, T, H, W)\n",
    "\n",
    "        # Wybieramy np. 16 klatek\n",
    "        video = sample_frames(video, num_frames=16)\n",
    "        \n",
    "        if self.transform:\n",
    "            video = self.transform(video)\n",
    "\n",
    "\n",
    "        return video, label\n",
    "        # print(video)\n",
    "        # print(video.shape)\n",
    "\n",
    "        # num_frames = video.shape[0]  # Liczba klatek w wideo\n",
    "        # frames = list(video.numpy())  # Konwersja na listę, aby można było dodać padding\n",
    "\n",
    "\n",
    "\n",
    "        # # Dodanie paddingu, jeśli mniej niż max_frames\n",
    "        # if num_frames < self.max_frames:\n",
    "        #     pad = [np.zeros_like(frames[0])] * (self.max_frames - num_frames)\n",
    "        #     frames.extend(pad)\n",
    "        #     mask = [1] * num_frames + [0] * (self.max_frames - num_frames)\n",
    "        # else:\n",
    "        #     frames = frames[:self.max_frames]  # Odcinamy nadmiar\n",
    "        #     mask = [1] * self.max_frames\n",
    "\n",
    "        # # Konwersja do tensora PyTorch\n",
    "        # frames_tensor = torch.tensor(frames, dtype=torch.float32).permute(3, 0, 1, 2)  # (C, T, H, W)\n",
    "        # mask_tensor = torch.tensor(mask, dtype=torch.float32)  # (T,)\n",
    "\n",
    "        # if self.transform:\n",
    "        #     frames_tensor = self.transform(frames_tensor)\n",
    "\n",
    "        # return frames_tensor, label, mask_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "Label: content\n"
     ]
    }
   ],
   "source": [
    "# Ścieżka do folderu z danymi\n",
    "root_dir = \"dataset\"\n",
    "\n",
    "# Tworzenie instancji datasetu dla treningu\n",
    "train_dataset = AdDetectionDataset(root=root_dir, split=\"train\")\n",
    "\n",
    "\n",
    "\n",
    "#Sprawdzenie\n",
    "print(len(train_dataset))\n",
    "\n",
    "# Uzyskiwanie jednego elementu\n",
    "video, label= train_dataset[0]\n",
    "print(f\"Label: {train_dataset.map_idx_to_label(label)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4  \n",
    "num_workers = 0  # Liczba wątków do wczytywania danych - dla Windows 0 bo jest problem z wielowatkowscia wynikajaca z działania multiprocessingu\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "#Załadowanie modelu\n",
    "# MODEL_NAME = \"vit_base_patch16_224\"\n",
    "# model = create_model(MODEL_NAME, pretrained=True, num_classes=400).to(device)\n",
    "# model.eval()\n",
    "\n",
    "# processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k600\")\n",
    "# model = TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k600\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for 'C:\\Users\\User\\.cache\\huggingface\\hub\\models--facebook--timesformer-base-finetuned-k400\\snapshots\\8aaf40ea7d3d282dcb0a5dea01a198320d15d6c0\\pytorch_model.bin' at 'C:\\Users\\User\\.cache\\huggingface\\hub\\models--facebook--timesformer-base-finetuned-k400\\snapshots\\8aaf40ea7d3d282dcb0a5dea01a198320d15d6c0\\pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\User\\CV_project\\env\\lib\\site-packages\\transformers\\modeling_utils.py:535\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[0;32m    534\u001b[0m     weights_only_kwarg \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: weights_only}\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    536\u001b[0m         checkpoint_file,\n\u001b[0;32m    537\u001b[0m         map_location\u001b[38;5;241m=\u001b[39mmap_location,\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mweights_only_kwarg,\n\u001b[0;32m    539\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_args,\n\u001b[0;32m    540\u001b[0m     )\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\User\\CV_project\\env\\lib\\site-packages\\torch\\serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    711\u001b[0m module \u001b[38;5;241m=\u001b[39m _import_dotted_name(storage_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m)\n\u001b[1;32m--> 712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, storage_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\User\\CV_project\\env\\lib\\site-packages\\torch\\serialization.py:1047\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1039\u001b[0m location = location_tag(storage)\n\u001b[0;32m   1041\u001b[0m # TODO: There's an issue here with FC. It might be impossible to\n\u001b[0;32m   1042\u001b[0m # solve, but it's worth noting. Imagine we save a list `[storage,\n\u001b[0;32m   1043\u001b[0m # tensor]`, where `tensor.storage()` is the same as `storage`, and\n\u001b[0;32m   1044\u001b[0m # `tensor.element_size() > 1`. Let's say that `tensor.dtype ==\n\u001b[0;32m   1045\u001b[0m # torch.float`.  The storage will be serialized with element size\n\u001b[0;32m   1046\u001b[0m # of 1, since we're choosing to serialize the first occurance of\n\u001b[1;32m-> 1047\u001b[0m # a duplicate storage. Since this legacy serialization format saves\n\u001b[0;32m   1048\u001b[0m # the numel of the storage, rather than nbytes directly, we'll be\n\u001b[0;32m   1049\u001b[0m # effectively saving nbytes in this case.  We'll be able to load it\n\u001b[0;32m   1050\u001b[0m # and the tensor back up with no problems in _this_ and future\n\u001b[0;32m   1051\u001b[0m # versions of pytorch, but in older versions, here's the problem:\n\u001b[0;32m   1052\u001b[0m # the storage will be loaded up as a UntypedStorage, and then the\n\u001b[0;32m   1053\u001b[0m # FloatTensor will loaded and the UntypedStorage will be assigned to\n\u001b[0;32m   1054\u001b[0m # it. Since the storage dtype does not match the tensor dtype, this\n\u001b[0;32m   1055\u001b[0m # will cause an error.  If we reverse the list, like `[tensor,\n\u001b[0;32m   1056\u001b[0m # storage]`, then we will save the `tensor.storage()` as a faked\n\u001b[0;32m   1057\u001b[0m # `FloatStorage`, and the saved size will be the correct\n\u001b[0;32m   1058\u001b[0m # dtype-specific numel count that old versions expect. `tensor`\n\u001b[0;32m   1059\u001b[0m # will be able to load up properly in old versions, pointing to\n\u001b[0;32m   1060\u001b[0m # a FloatStorage. However, `storage` is still being translated to\n\u001b[0;32m   1061\u001b[0m # a UntypedStorage, and it will try to resolve to the same\n\u001b[0;32m   1062\u001b[0m # FloatStorage that `tensor` contains. This will also cause an\n\u001b[0;32m   1063\u001b[0m # error. It doesn't seem like there's any way around this.\n\u001b[0;32m   1064\u001b[0m # Probably, we just cannot maintain FC for the legacy format if the\n\u001b[0;32m   1065\u001b[0m # saved list contains both a tensor and a storage that point to the\n\u001b[0;32m   1066\u001b[0m # same data.  We should still be able to maintain FC for lists of\n\u001b[0;32m   1067\u001b[0m # just tensors, as long as all views share the same dtype as the\n\u001b[0;32m   1068\u001b[0m # tensor they are viewing.\n\u001b[0;32m   1070\u001b[0m if storage_key not in serialized_storages:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'weights_only' is an invalid keyword argument for Unpickler()",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\User\\CV_project\\env\\lib\\site-packages\\transformers\\modeling_utils.py:544\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(checkpoint_file) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    546\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou seem to have cloned a repository without having git-lfs installed. Please install \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    547\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit-lfs and run `git lfs install` followed by `git lfs pull` in the folder \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    548\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou cloned.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    549\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\encodings\\cp1250.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 1963: character maps to <undefined>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m   \u001b[38;5;66;03m#\"sequential\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# model = TimesformerForVideoClassification.from_pretrained(\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#     \"facebook/timesformer-base-finetuned-k600\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTimesformerForVideoClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/timesformer-base-finetuned-k400\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\User\\CV_project\\env\\lib\\site-packages\\transformers\\modeling_utils.py:3993\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pt:\n\u001b[0;32m   3991\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded \u001b[38;5;129;01mand\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3992\u001b[0m         \u001b[38;5;66;03m# Time to load the checkpoint\u001b[39;00m\n\u001b[1;32m-> 3993\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3995\u001b[0m     \u001b[38;5;66;03m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[0;32m   3996\u001b[0m     \u001b[38;5;66;03m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[0;32m   3997\u001b[0m     \u001b[38;5;66;03m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[0;32m   3998\u001b[0m     \u001b[38;5;66;03m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[0;32m   3999\u001b[0m     \u001b[38;5;66;03m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[0;32m   4000\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\CV_project\\env\\lib\\site-packages\\transformers\\modeling_utils.py:556\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    552\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to locate the file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is necessary to load this pretrained \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    553\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel. Make sure you have saved the model properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load weights from pytorch checkpoint file for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for 'C:\\Users\\User\\.cache\\huggingface\\hub\\models--facebook--timesformer-base-finetuned-k400\\snapshots\\8aaf40ea7d3d282dcb0a5dea01a198320d15d6c0\\pytorch_model.bin' at 'C:\\Users\\User\\.cache\\huggingface\\hub\\models--facebook--timesformer-base-finetuned-k400\\snapshots\\8aaf40ea7d3d282dcb0a5dea01a198320d15d6c0\\pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
     ]
    }
   ],
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16  \n",
    "# )\n",
    "\n",
    "device_map=\"auto\"   #\"sequential\"\n",
    "# model = TimesformerForVideoClassification.from_pretrained(\n",
    "#     \"facebook/timesformer-base-finetuned-k600\"\n",
    "# )\n",
    "model = TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\", force_download=True)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch videos shape: torch.Size([4, 3, 100, 360, 640])\n",
      "Batch labels: tensor([1, 1, 0, 1])\n",
      "Batch masks shape: torch.Size([4, 100])\n"
     ]
    }
   ],
   "source": [
    "for batch_videos, batch_labels, batch_masks in train_loader:\n",
    "    batch_videos, batch_labels, batch_masks = batch_videos.to(device), batch_labels.to(device), batch_masks.to(device)\n",
    "    print(f\"Batch videos shape: {batch_videos.shape}\")  # (B, C, T, H, W)\n",
    "    print(f\"Batch labels: {batch_labels}\")\n",
    "    print(f\"Batch masks shape: {batch_masks.shape}\")  # (B, T)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m2\u001b[39m, batch_videos\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])):  \u001b[38;5;66;03m# Wybieramy maksymalnie 2 filmy\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     frames \u001b[38;5;241m=\u001b[39m batch_videos[i]  \u001b[38;5;66;03m# Wybór filmu\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     predicted_class, confidence \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎥 Wideo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m → Klasa: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Pewność: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidence\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m \n",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m, in \u001b[0;36mclassify_video\u001b[1;34m(frames)\u001b[0m\n\u001b[0;32m      3\u001b[0m frames \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# (1, T, C, H, W)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 5\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\User\\CV_project\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\User\\CV_project\\env\\lib\\site-packages\\timm\\models\\vision_transformer.py:849\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 849\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    850\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[0;32m    851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\User\\CV_project\\env\\lib\\site-packages\\timm\\models\\vision_transformer.py:823\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 823\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    824\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos_embed(x)\n\u001b[0;32m    825\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_drop(x)\n",
      "File \u001b[1;32mc:\\Users\\User\\CV_project\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\User\\CV_project\\env\\lib\\site-packages\\timm\\layers\\patch_embed.py:113\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 113\u001b[0m     B, C, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrict_img_size:\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
